{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from embargo_purge import get_embargo_table, embargo, purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t0\n",
       "2017-01-02    2017-02-12\n",
       "2017-02-03    2017-03-13\n",
       "2017-03-04    2017-04-14\n",
       "2017-04-05    2017-05-15\n",
       "2017-05-06    2017-06-16\n",
       "2017-06-07    2017-07-17\n",
       "2017-07-08    2017-08-18\n",
       "2017-08-09    2017-09-19\n",
       "2017-09-10    2017-10-11\n",
       "2017-10-11    2017-11-12\n",
       "Name: t1, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv(os.path.join('../_data/', 'AFML Ch7.csv'),\n",
    "                       index_col=0)\n",
    "\n",
    "# Change to Series\n",
    "all_times = raw_data['t1']\n",
    "all_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t0\n",
       "2017-05-06    2017-06-16\n",
       "2017-06-07    2017-07-17\n",
       "2017-07-08    2017-08-18\n",
       "Name: t1, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_times = all_times.iloc[4:7]\n",
    "test_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t0\n",
       "2017-01-02    2017-02-12\n",
       "2017-02-03    2017-03-13\n",
       "2017-03-04    2017-04-14\n",
       "2017-09-10    2017-10-11\n",
       "2017-10-11    2017-11-12\n",
       "Name: t1, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purged_train_times = purge(all_times, test_times)\n",
    "purged_train_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t0\n",
       "2017-01-02    2017-03-04\n",
       "2017-02-03    2017-04-05\n",
       "2017-03-04    2017-05-06\n",
       "2017-04-05    2017-06-07\n",
       "2017-05-06    2017-07-08\n",
       "2017-06-07    2017-08-09\n",
       "2017-07-08    2017-09-10\n",
       "2017-08-09    2017-10-11\n",
       "2017-09-10    2017-10-11\n",
       "2017-10-11    2017-10-11\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embargo_table = get_embargo_table(all_times.index, embargo_pct=.2)\n",
    "embargo_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t0\n",
       "2017-01-02    2017-02-12\n",
       "2017-02-03    2017-03-13\n",
       "2017-03-04    2017-04-14\n",
       "2017-04-05    2017-05-15\n",
       "2017-10-11    2017-11-12\n",
       "Name: t1, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embargoed_times = embargo(all_times, test_times, embargo_table)\n",
    "embargoed_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t0\n",
       "2017-01-02    2017-02-12\n",
       "2017-02-03    2017-03-13\n",
       "2017-03-04    2017-04-14\n",
       "2017-10-11    2017-11-12\n",
       "Name: t1, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purged_train_times = purge(embargoed_times, test_times)\n",
    "purged_train_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hyperparameter tuning of a financial ML model by PCV(Purged Cross-Validation).\n",
    "Expanded scikit-learn's k-fold cross-validation to \"embargo\" and \"purge\" the training set.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection._split import _BaseKFold\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "\n",
    "class PurgedKFold(_BaseKFold):\n",
    "    \"\"\"\n",
    "    Extend KFold to work with labels that span intervals.\n",
    "    Training set is embargoed, then purged of observations overlapping test-label intervals.\n",
    "    Test set is assumed contiguous (shuffle=False) w/o training example in between.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_splits=3, event_times=None, embargo_pct=.01):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_splits(int): number of folds\n",
    "            event_times(Series): endtime of each event\n",
    "            embargo_pct(float): percentage of bars to embargo\n",
    "        \"\"\"\n",
    "        assert(isinstance(event_times, pd.Series)), 'event_times must be a pandas Series!!!'\n",
    "        super(PurgedKFold, self).__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.event_times = event_times\n",
    "        self.embargo_pct = embargo_pct\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"\n",
    "        Generate training/test indices for PCV.\n",
    "        Args:\n",
    "            X(DataFrame): Features for ML\n",
    "            y(Series): Labels for ML (for consistency with other objects in pipeline)\n",
    "            groups:\n",
    "\n",
    "        Returns: generator of training/test indices\n",
    "        \"\"\"\n",
    "        if (X.index == self.event_times.index).sum() != len(self.event_times):\n",
    "            raise ValueError('X and ThruDateValues must have the same index.')\n",
    "        indices = np.arange(X.shape[0])\n",
    "        mbrg = int(X.shape[0] * self.embargo_pct)\n",
    "        test_starts = [(i[0], i[-1] + 1) for i in\n",
    "                       np.array_split(np.arange(X.shape[0]), self.n_splits)]\n",
    "        for i, j in test_starts:\n",
    "            t0 = self.event_times.index[i]  # start of test set\n",
    "            test_indices = indices[i:j]\n",
    "            maxT1Idx = self.event_times.index.searchsorted(self.event_times[test_indices].max())\n",
    "            train_indices = self.event_times.index.searchsorted(\n",
    "                self.event_times[self.event_times <= t0].index)\n",
    "            train_indices = np.concatenate((train_indices, indices[maxT1Idx+mbrg:]))\n",
    "            yield train_indices, test_indices\n",
    "\n",
    "\n",
    "def cv_score(clf, X, y, sample_weight, scoring='neg_log_loss',\n",
    "             event_times=None, n_splits=None, cv_gen=None, embargo_pct=.01):\n",
    "    \"\"\"\n",
    "    Using class PurgedKFold to fix bug in scikit-learn's cross_val_score()\n",
    "    https://github.com/scikit-learn/scikit-learn/issues/9144\n",
    "\n",
    "    Args:\n",
    "        clf(Pipeline): classifier\n",
    "        X(DataFrame): features\n",
    "        y(Series): labels\n",
    "        sample_weight(?): will specify later\n",
    "        scoring(str): performance metric\n",
    "        event_times(Series): endtime of each event\n",
    "        n_splits(int): number of folds\n",
    "        cv_gen: k-fold cross-validation object\n",
    "        embargo_pct(float): percentage of bars to embargo\n",
    "\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if scoring not in ['neg_log_loss', 'accuracy']:\n",
    "        raise Exception('wrong scoring method.')\n",
    "\n",
    "    if cv_gen is None:\n",
    "        cv_gen = PurgedKFold(n_splits=n_splits, event_times=event_times,\n",
    "                             embargo_pct=embargo_pct)  # Also embargoed of course\n",
    "\n",
    "    score = []\n",
    "    for train, test in cv_gen.split(X=X):\n",
    "        fit = clf.fit(X=X.iloc[train, :], y=y.iloc[train],\n",
    "                      sample_weight=sample_weight.iloc[train].values)\n",
    "        if scoring == 'neg_log_loss':\n",
    "            prob = fit.predict_proba(X.iloc[test, :])\n",
    "            _score = -log_loss(y.iloc[test], prob,\n",
    "                               sample_weight=sample_weight.iloc[test].values, labels=clf.classes_)\n",
    "        else:\n",
    "            pred = fit.predict(X.iloc[test, :])\n",
    "            _score = accuracy_score(y.iloc[test], pred,\n",
    "                                    sample_weight=sample_weight.iloc[test].values)\n",
    "        score.append(_score)\n",
    "    return np.array(score)\n",
    "\n",
    "\n",
    "class WeightedPipeline(Pipeline):\n",
    "    \"\"\"\n",
    "    Expand scikit-learn's Pipeline to accept sample_weight argument.\n",
    "    Used in clf_hyper_fit() when bagging is done.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y, sample_weight=None, **fit_params):\n",
    "        if sample_weight is not None:\n",
    "            fit_params[self.steps[-1][0] + '__sample_weight'] = sample_weight\n",
    "\n",
    "        return super(WeightedPipeline, self).fit(X, y, **fit_params)\n",
    "\n",
    "\n",
    "def clf_hyper_fit(X, y, event_times, pipe_clf, param_grid, n_rand_iter=0, is_meta=False,\n",
    "                  embargo_pct=.01, bagging=[0, None, 1.], n_splits=3, n_jobs=-1, **fit_params):\n",
    "    \"\"\"\n",
    "    Perform GridSearchCV or RandomizedSearchCV with purged k-fold cross-validation\n",
    "\n",
    "    Args:\n",
    "        X(DataFrame): features\n",
    "        y(Series): labels\n",
    "        event_times(Series): endtime of each event\n",
    "        pipe_clf(Pipeline): scikit-learn's pipeline\n",
    "        param_grid(): grid of hyperparameters to search for tuning\n",
    "        n_rand_iter(int): RandomSearchCV iterations to perform. If 0, GridSearchCV\n",
    "        is_meta(bool): if True, meta-labeling\n",
    "        embargo_pct(float): percentage of bars to embargo\n",
    "        bagging(list): option to control bagging\n",
    "        n_splits(int): number of folds\n",
    "        n_jobs(int): number of concurrent jobs\n",
    "        **fit_params():\n",
    "\n",
    "    Returns:\n",
    "        search(Pipeline): the best performing estimator found by tuning\n",
    "    \"\"\"\n",
    "    # 'f1' for balancing recall vs precision; 'neg_log_loss' to be symmetric towards all labels\n",
    "    scoring = 'f1' if is_meta else 'neg_log_loss'\n",
    "\n",
    "    # Hyperparameter search on training data\n",
    "    purged_cv = PurgedKFold(n_splits=n_splits, event_times=event_times, embargo_pct=embargo_pct)\n",
    "    if n_rand_iter == 0:\n",
    "        search = GridSearchCV(estimator=pipe_clf, param_grid=param_grid,\n",
    "                              scoring=scoring, cv=purged_cv, n_jobs=n_jobs, iid=False)\n",
    "    else:\n",
    "        search = RandomizedSearchCV(estimator=pipe_clf, param_distributions=param_grid,\n",
    "                                    scoring=scoring, cv=purged_cv, n_jobs=n_jobs, iid=False,\n",
    "                                    n_iter=n_rand_iter)\n",
    "\n",
    "    search = search.fit(X, y, **fit_params).best_estimator_  # Pipeline object\n",
    "\n",
    "    # Fit validated model on the entirety of the data\n",
    "    if bagging[1] > 0:\n",
    "        search = BaggingClassifier(\n",
    "            base_estimator=WeightedPipeline(search.steps), n_estimators=int(bagging[0]),\n",
    "            max_samples=float(bagging[1]), max_features=float(bagging[2]), n_jobs=n_jobs)\n",
    "        search = search.fit(X, y,\n",
    "                            sample_weight=fit_params[search.base_estimator.steps[-1][0]\n",
    "                                                     + '__sample_weight'])\n",
    "        search = Pipeline([('bag', search)])\n",
    "\n",
    "    return search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
